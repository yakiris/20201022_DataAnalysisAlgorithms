{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тема: Алгоритм линейной регрессии. Градиентный спуск."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[ 1,  1],\n",
    "              [ 1,  1],\n",
    "              [ 1,  2],\n",
    "              [ 1,  5],\n",
    "              [ 1,  3],\n",
    "              [ 1,  0],\n",
    "              [ 1,  5],\n",
    "              [ 1, 10],\n",
    "              [ 1,  1],\n",
    "              [ 1,  2]])\n",
    "\n",
    "y = [45, 55, 50, 55, 60, 35, 75, 80, 50, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mse(y, y_pred):\n",
    "    err = np.mean((y - y_pred)**2)\n",
    "    return err\n",
    "\n",
    "def mserror(X, w, y):\n",
    "    y_pred = X.dot(w)\n",
    "    return (np.sum((y_pred - y)**2)) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Подберите скорость обучения (eta) и количество итераций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects = 2        \n",
      "Learning rate = 0.001        \n",
      "Initial weights = [1.  0.5] \n",
      "\n",
      "Iteration #0: W_new = [1.54  2.385], MSE = 3047.75\n",
      "Iteration #500: W_new = [39.81400584  4.76423349], MSE = 57.05\n",
      "Iteration #1000: W_new = [44.26788718  3.95659078], MSE = 44.27\n",
      "Iteration #1500: W_new = [44.91963069  3.83840715], MSE = 43.98\n",
      "Iteration #2000: W_new = [45.03246864  3.81794572], MSE = 43.97\n",
      "Iteration #2500: W_new = [45.05522426  3.81381934], MSE = 43.97\n",
      "Iteration #3000: W_new = [45.06049449  3.81286367], MSE = 43.97\n",
      "Iteration #3500: W_new = [45.0618784   3.81261272], MSE = 43.97\n",
      "Iteration #4000: W_new = [45.06228567  3.81253887], MSE = 43.97\n",
      "Iteration #4500: W_new = [45.06241858  3.81251476], MSE = 43.97\n",
      "Iteration #5000: W_new = [45.06246622  3.81250613], MSE = 43.97\n",
      "Iteration #5500: W_new = [45.06248482  3.81250275], MSE = 43.97\n",
      "Iteration #6000: W_new = [45.06249266  3.81250133], MSE = 43.97\n",
      "Iteration #6500: W_new = [45.06249621  3.81250069], MSE = 43.97\n"
     ]
    }
   ],
   "source": [
    "n = X.shape[1]\n",
    "\n",
    "# ИЗМЕНЕНИЯ\n",
    "# eta = 1e-2\n",
    "# n_iter = 100\n",
    "eta = 1e-3\n",
    "n_iter = 6501\n",
    "# ИЗМЕНЕНИЯ\n",
    "\n",
    "W = np.array([1, 0.5])\n",
    "print(f'Number of objects = {n} \\\n",
    "       \\nLearning rate = {eta} \\\n",
    "       \\nInitial weights = {W} \\n')\n",
    "\n",
    "for i in range(n_iter):\n",
    "    y_pred = np.dot(X, W)\n",
    "    err = calc_mse(y, y_pred)\n",
    "    for k in range(W.shape[0]):\n",
    "        W[k] -= eta * (1/n * 2 * X[:, k] @ (y_pred - y))\n",
    "# ИЗМЕНЕНИЯ\n",
    "#     if i % 10 == 0:\n",
    "    if i % 500 == 0:\n",
    "# ИЗМЕНЕНИЯ\n",
    "        eta /= 1.1\n",
    "        print(f'Iteration #{i}: W_new = {W}, MSE = {round(err, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2*. В этом коде мы избавляемся от итераций по весам, но тут есть ошибка, исправьте ее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects = 2        \n",
      "Learning rate = 0.01        \n",
      "Initial weights = [1.  0.5] \n",
      "\n",
      "Iteration #0: W_new = [ 6.4  19.35], MSE = 3047.75\n",
      "Iteration #10: W_new = [19.96966932  8.87179137], MSE = 379.6\n",
      "Iteration #20: W_new = [29.27719328  6.70531799], MSE = 172.93\n",
      "Iteration #30: W_new = [35.16120834  5.60976141], MSE = 94.66\n",
      "Iteration #40: W_new = [38.85365942  4.93848587], MSE = 63.9\n",
      "Iteration #50: W_new = [41.16920131  4.51849616], MSE = 51.81\n",
      "Iteration #60: W_new = [42.62118483  4.25519525], MSE = 47.05\n",
      "Iteration #70: W_new = [43.53165976  4.0900943 ], MSE = 45.18\n",
      "Iteration #80: W_new = [44.10257814  3.98656703], MSE = 44.45\n",
      "Iteration #90: W_new = [44.46057568  3.9216497 ], MSE = 44.16\n"
     ]
    }
   ],
   "source": [
    "n = X.shape[1]\n",
    "\n",
    "eta = 1e-2 \n",
    "n_iter = 100\n",
    "\n",
    "W = np.array([1, 0.5])\n",
    "print(f'Number of objects = {n} \\\n",
    "       \\nLearning rate = {eta} \\\n",
    "       \\nInitial weights = {W} \\n')\n",
    "\n",
    "for i in range(n_iter):\n",
    "    y_pred = np.dot(X, W)\n",
    "    err = calc_mse(y, y_pred)\n",
    "#     for k in range(W.shape[0]):\n",
    "#         W[k] -= eta * (1/n * 2 * X[:, k] @ (y_pred - y))\n",
    "# ИЗМЕНЕНИЯ\n",
    "#     W -= eta * (1/n * 2 * np.dot(X, y_pred - y))\n",
    "    W -= eta * (1/n * 2 * np.dot(X.T, y_pred - y))\n",
    "# ИЗМЕНЕНИЯ\n",
    "    if i % 10 == 0:\n",
    "        print(f'Iteration #{i}: W_new = {W}, MSE = {round(err,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3*. Вместо того, чтобы задавать количество итераций, задайте другое условие останова алгоритма - когда веса перестают изменяться меньше определенного порога $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects = 2        \n",
      "Learning rate = 0.01        \n",
      "Initial weights = [1.  0.5] \n",
      "\n",
      "Iter 0: error - 2038.4825, weights: [ 6.4  19.35]\n",
      "Iter 1: error - 1438.6889000000006, weights: [5.605 4.535]\n",
      "Iter 2: error - 1074.1986440000005, weights: [ 9.334 15.144]\n",
      "Iter 3: error - 845.6860673600004, weights: [9.5074 6.599 ]\n",
      "Iter 4: error - 696.4392026431999, weights: [12.22696 12.52848]\n",
      "Iter 5: error - 594.014762592512, weights: [12.89572   7.561976]\n",
      "Iter 6: error - 519.7961659866829, weights: [14.9875552 10.8379008]\n",
      "Iter 7: error - 463.04947273233245, weights: [15.88742944  7.91720288]\n",
      "Iter 8: error - 417.5392871963556, weights: [17.57352563  9.69172915]\n",
      "Iter 9: error - 379.6016983861652, weights: [18.55865432  7.9437319 ]\n",
      "Iter 10: error - 347.04654712893375, weights: [19.96966932  8.87179137]\n",
      "Iter 11: error - 318.5317693074339, weights: [20.96116498  7.79884524]\n",
      "Iter 12: error - 293.2064885433628, weights: [22.17539491  8.25245884]\n",
      "Iter 13: error - 270.5071338352033, weights: [23.13211776  7.57066034]\n",
      "Iter 14: error - 250.0407144152217, weights: [24.19770788  7.76090243]\n",
      "Iter 15: error - 231.51775915435869, weights: [25.09966637  7.30805593]\n",
      "Iter 16: error - 214.71357719559992, weights: [26.04728295  7.35446094]\n",
      "Iter 17: error - 199.4456879038683, weights: [26.88621637  7.03769246]\n",
      "Iter 18: error - 185.56049967952612, weights: [27.736287    7.00775037]\n",
      "Iter 19: error - 172.92529491633326, weights: [28.51033319  6.77368864]\n",
      "Iter 20: error - 161.42327344230694, weights: [29.27719328  6.70531799]\n",
      "Iter 21: error - 150.95037180998716, weights: [29.98787855  6.52311942]\n",
      "Iter 22: error - 141.41312532628908, weights: [30.68215487  6.43745284]\n",
      "Iter 23: error - 132.72715273661044, weights: [31.33270353  6.28913655]\n",
      "Iter 24: error - 124.8160218810352, weights: [31.96269221  6.19779335]\n",
      "Iter 25: error - 117.61035640527325, weights: [32.55708499  6.07273699]\n",
      "Iter 26: error - 111.04710173667573, weights: [33.12955539  5.98195861]\n",
      "Iter 27: error - 105.06890180978648, weights: [33.67201227  5.87376235]\n",
      "Iter 28: error - 99.62355713819129, weights: [34.19268233  5.78676267]\n",
      "Iter 29: error - 94.66354586632977, weights: [34.6873853   5.69146143]\n",
      "Iter 30: error - 90.14559586172967, weights: [35.16120834  5.60976141]\n",
      "Iter 31: error - 86.03029969825462, weights: [35.61215908  5.52480451]\n",
      "Iter 32: error - 82.28176665869617, weights: [36.04350182  5.44898912]\n",
      "Iter 33: error - 78.8673072912951, weights: [36.4544549   5.37265707]\n",
      "Iter 34: error - 75.7571469552487, weights: [36.84721229  5.30280358]\n",
      "Iter 35: error - 72.92416539396066, weights: [37.22164999  5.23387381]\n",
      "Iter 36: error - 70.3436598014609, weights: [37.57932285  5.16979334]\n",
      "Iter 37: error - 67.99312916593286, weights: [37.92045256  5.10734781]\n",
      "Iter 38: error - 65.85207792450333, weights: [38.24620296  5.04872077]\n",
      "Iter 39: error - 63.90183716864531, weights: [38.55696644  4.99203458]\n",
      "Iter 40: error - 62.12540181353258, weights: [38.85365942  4.93848587]\n",
      "Iter 41: error - 60.50728229580248, weights: [39.13674772  4.88696207]\n",
      "Iter 42: error - 59.03336949765238, weights: [39.40698433  4.83810224]\n",
      "Iter 43: error - 57.69081171438155, weights: [39.66485522  4.79123314]\n",
      "Iter 44: error - 56.46790258970166, weights: [39.91099976  4.74668024]\n",
      "Iter 45: error - 55.35397904002307, weights: [40.14589571  4.70402391]\n",
      "Iter 46: error - 54.33932827673279, weights: [40.37009897  4.66341455]\n",
      "Iter 47: error - 53.41510311521658, weights: [40.58406471  4.62458012]\n",
      "Iter 48: error - 52.57324483186223, weights: [40.7882842  4.5875745]\n",
      "Iter 49: error - 51.80641289622787, weights: [40.98318343  4.55221259]\n",
      "Iter 50: error - 51.10792096557984, weights: [41.16920131  4.51849616]\n",
      "Iter 51: error - 50.471678583653656, weights: [41.34673233  4.4862923 ]\n",
      "Iter 52: error - 49.89213807525083, weights: [41.51617141  4.45557569]\n",
      "Iter 53: error - 49.364246173605125, weights: [41.67788156  4.42624559]\n",
      "Iter 54: error - 48.88339995872561, weights: [41.83221973  4.39826362]\n",
      "Iter 55: error - 48.44540672251698, weights: [41.97951867  4.37154955]\n",
      "Iter 56: error - 48.04644741072177, weights: [42.12010194  4.34605971]\n",
      "Iter 57: error - 47.683043322915424, weights: [42.25427383  4.32172762]\n",
      "Iter 58: error - 47.35202578019666, weights: [42.38232816  4.29850852]\n",
      "Iter 59: error - 47.050508496091695, weights: [42.50454279  4.27634559]\n",
      "Iter 60: error - 46.77586240976177, weights: [42.62118483  4.25519525]\n",
      "Iter 61: error - 46.52569276207347, weights: [42.73250777  4.23500787]\n",
      "Iter 62: error - 46.297818214648224, weights: [42.83875463  4.21574216]\n",
      "Iter 63: error - 46.09025182982051, weights: [42.94015652  4.1973541 ]\n",
      "Iter 64: error - 45.9011837456607, weights: [43.03693464  4.17980517]\n",
      "Iter 65: error - 45.72896539499889, weights: [43.12929963  4.16305599]\n",
      "Iter 66: error - 45.572095130847856, weights: [43.21745287  4.14707092]\n",
      "Iter 67: error - 45.4292051328874, weights: [43.3015863   4.13181449]\n",
      "Iter 68: error - 45.299049480841816, weights: [43.38188333  4.11725396]\n",
      "Iter 69: error - 45.18049329075707, weights: [43.4585188   4.10335723]\n",
      "Iter 70: error - 45.072502819452055, weights: [43.53165976  4.0900943 ]\n",
      "Iter 71: error - 44.97413645086059, weights: [43.60146549  4.07743606]\n",
      "Iter 72: error - 44.884536485669706, weights: [43.66808812  4.06535511]\n",
      "Iter 73: error - 44.80292166266505, weights: [43.73167278  4.05382499]\n",
      "Iter 74: error - 44.72858034657311, weights: [43.792358    4.04282068]\n",
      "Iter 75: error - 44.66086432300263, weights: [43.850276    4.03231813]\n",
      "Iter 76: error - 44.59918314638039, weights: [43.90555296  4.02229451]\n",
      "Iter 77: error - 44.542998991598424, weights: [43.95830931  4.01272795]\n",
      "Iter 78: error - 44.491821964482064, weights: [44.00866     4.00359764]\n",
      "Iter 79: error - 44.44520583018881, weights: [44.0567147   3.99488365]\n",
      "Iter 80: error - 44.402744122291956, weights: [44.10257814  3.98656703]\n",
      "Iter 81: error - 44.364066598622216, weights: [44.14635021  3.97862964]\n",
      "Iter 82: error - 44.32883601296473, weights: [44.1881263   3.97105419]\n",
      "Iter 83: error - 44.29674517446203, weights: [44.22799741  3.96382418]\n",
      "Iter 84: error - 44.2675142690829, weights: [44.26605042  3.95692385]\n",
      "Iter 85: error - 44.24088841980158, weights: [44.30236822  3.95033818]\n",
      "Iter 86: error - 44.216635464213766, weights: [44.33702995  3.94405281]\n",
      "Iter 87: error - 44.194543930211225, weights: [44.37011111  3.93805405]\n",
      "Iter 88: error - 44.174421192064194, weights: [44.40168378  3.93232883]\n",
      "Iter 89: error - 44.156091790833514, weights: [44.43181676  3.92686468]\n",
      "Iter 90: error - 44.13939590446752, weights: [44.46057568  3.9216497 ]\n",
      "Iter 91: error - 44.12418795424382, weights: [44.4880232   3.91667251]\n",
      "Iter 92: error - 44.11033533540491, weights: [44.51421913  3.91192228]\n",
      "Iter 93: error - 44.09771726091926, weights: [44.53922053  3.90738866]\n",
      "Iter 94: error - 44.0862237082866, weights: [44.56308188  3.90306178]\n",
      "Iter 95: error - 44.07575446020364, weights: [44.58585516  3.89893219]\n",
      "Iter 96: error - 44.06621823072561, weights: [44.60758998  3.89499092]\n",
      "Iter 97: error - 44.05753186930433, weights: [44.62833371  3.89122936]\n",
      "Iter 98: error - 44.049619635762255, weights: [44.64813153  3.88763933]\n",
      "Iter 99: error - 44.04241253988077, weights: [44.66702658  3.88421301]\n",
      "Iter 100: error - 44.03584773984438, weights: [44.68506002  3.88094292]\n",
      "Iter 101: error - 44.02986799429549, weights: [44.70227114  3.87782195]\n",
      "Iter 102: error - 44.024421163222016, weights: [44.71869744  3.87484329]\n",
      "Iter 103: error - 44.01945975332579, weights: [44.73437471  3.87200046]\n",
      "Iter 104: error - 44.01494050390787, weights: [44.7493371   3.86928726]\n",
      "Iter 105: error - 44.01082400965957, weights: [44.76361721  3.86669779]\n",
      "Iter 106: error - 44.00707437707031, weights: [44.77724615  3.86422639]\n",
      "Iter 107: error - 44.003658911456554, weights: [44.79025362  3.86186768]\n",
      "Iter 108: error - 44.00054783188244, weights: [44.80266795  3.85961654]\n",
      "Iter 109: error - 43.99771401148693, weights: [44.8145162   3.85746804]\n",
      "Iter 110: error - 43.995132740952805, weights: [44.82582417  3.85541751]\n",
      "Iter 111: error - 43.99278151305532, weights: [44.8366165   3.85346049]\n",
      "Iter 112: error - 43.990639826411964, weights: [44.8469167   3.85159271]\n",
      "Iter 113: error - 43.988689006721806, weights: [44.85674722  3.84981009]\n",
      "Iter 114: error - 43.986912043936314, weights: [44.86612947  3.84810877]\n",
      "Iter 115: error - 43.98529344394116, weights: [44.87508389  3.84648502]\n",
      "Iter 116: error - 43.98381909345634, weights: [44.88362999  3.84493532]\n",
      "Iter 117: error - 43.982476136976274, weights: [44.8917864   3.84345628]\n",
      "Iter 118: error - 43.98125286467713, weights: [44.89957088  3.84204468]\n",
      "Iter 119: error - 43.980138610313645, weights: [44.90700038  3.84069746]\n",
      "Iter 120: error - 43.97912365821558, weights: [44.91409111  3.83941166]\n",
      "Iter 121: error - 43.97819915857245, weights: [44.9208585  3.8381845]\n",
      "Iter 122: error - 43.977357050268274, weights: [44.9273173  3.8370133]\n",
      "Iter 123: error - 43.976589990593084, weights: [44.93348158  3.8358955 ]\n",
      "Iter 124: error - 43.9758912912187, weights: [44.93936477  3.83482868]\n",
      "Iter 125: error - 43.975254859880216, weights: [44.94497969  3.8338105 ]\n",
      "Iter 126: error - 43.97467514725494, weights: [44.95033857  3.83283875]\n",
      "Iter 127: error - 43.97414709857542, weights: [44.95545309  3.83191131]\n",
      "Iter 128: error - 43.97366610955466, weights: [44.96033439  3.83102616]\n",
      "Iter 129: error - 43.973227986239394, weights: [44.9649931   3.83018137]\n",
      "Iter 130: error - 43.97282890844117, weights: [44.96943938  3.82937511]\n",
      "Iter 131: error - 43.97246539642643, weights: [44.97368291  3.82860561]\n",
      "Iter 132: error - 43.97213428057521, weights: [44.97773294  3.8278712 ]\n",
      "Iter 133: error - 43.971832673743855, weights: [44.98159828  3.82717028]\n",
      "Iter 134: error - 43.97155794609071, weights: [44.98528737  3.82650132]\n",
      "Iter 135: error - 43.97130770214544, weights: [44.98880824  3.82586286]\n",
      "Iter 136: error - 43.971079759921835, weights: [44.99216855  3.82525352]\n",
      "Iter 137: error - 43.97087213189211, weights: [44.99537564  3.82467197]\n",
      "Iter 138: error - 43.970683007656845, weights: [44.99843649  3.82411693]\n",
      "Iter 139: error - 43.97051073815926, weights: [45.00135776  3.8235872 ]\n",
      "Iter 140: error - 43.970353821306404, weights: [45.00414582  3.82308163]\n",
      "Iter 141: error - 43.970210888871705, weights: [45.00680675  3.82259911]\n",
      "Iter 142: error - 43.97008069456489, weights: [45.00934634  3.8221386 ]\n",
      "Iter 143: error - 43.96996210316495, weights: [45.01177013  3.82169908]\n",
      "Iter 144: error - 43.96985408062169, weights: [45.01408339  3.82127961]\n",
      "Iter 145: error - 43.969755685039374, weights: [45.01629117  3.82087926]\n",
      "Iter 146: error - 43.969666058464, weights: [45.01839828  3.82049717]\n",
      "Iter 147: error - 43.96958441940231, weights: [45.0204093  3.8201325]\n",
      "Iter 148: error - 43.96951005600768, weights: [45.02232862  3.81978446]\n",
      "Iter 149: error - 43.9694423198732, weights: [45.02416042  3.81945229]\n",
      "Iter 150: error - 43.969380620377954, weights: [45.02590869  3.81913527]\n",
      "Iter 151: error - 43.96932441953712, weights: [45.02757724  3.8188327 ]\n",
      "Iter 152: error - 43.96927322731099, weights: [45.0291697   3.81854394]\n",
      "Iter 153: error - 43.969226597332245, weights: [45.03068955  3.81826833]\n",
      "Iter 154: error - 43.969184123013676, weights: [45.0321401  3.8180053]\n",
      "Iter 155: error - 43.969145434003224, weights: [45.0335245   3.81775426]\n",
      "Iter 156: error - 43.96911019295449, weights: [45.03484577  3.81751467]\n",
      "Iter 157: error - 43.969078092585356, weights: [45.03610679  3.817286  ]\n",
      "Iter 158: error - 43.96904885299872, weights: [45.03731031  3.81706776]\n",
      "Iter 159: error - 43.969022219241864, weights: [45.03845895  3.81685947]\n",
      "Iter 160: error - 43.968997959083424, weights: [45.03955522  3.81666068]\n",
      "Iter 161: error - 43.96897586098847, weights: [45.04060149  3.81647096]\n",
      "Iter 162: error - 43.96895573227411, weights: [45.04160005  3.81628988]\n",
      "Iter 163: error - 43.968937397429244, weights: [45.04255308  3.81611707]\n",
      "Iter 164: error - 43.96892069658439, weights: [45.04346266  3.81595213]\n",
      "Iter 165: error - 43.96890548411758, weights: [45.04433075  3.81579471]\n",
      "Iter 166: error - 43.96889162738467, weights: [45.04515926  3.81564448]\n",
      "Iter 167: error - 43.968879005562755, weights: [45.04594999  3.81550109]\n",
      "Iter 168: error - 43.96886750859666, weights: [45.04670467  3.81536424]\n",
      "Iter 169: error - 43.96885703623932, weights: [45.04742493  3.81523363]\n",
      "Iter 170: error - 43.9688474971777, weights: [45.04811235  3.81510898]\n",
      "Iter 171: error - 43.96883880823653, weights: [45.04876842  3.81499001]\n",
      "Iter 172: error - 43.96883089365313, weights: [45.04939457  3.81487647]\n",
      "Iter 173: error - 43.96882368441682, weights: [45.04999218  3.8147681 ]\n",
      "Iter 174: error - 43.96881711766713, weights: [45.05056253  3.81466468]\n",
      "Iter 175: error - 43.968811136145646, weights: [45.05110687  3.81456597]\n",
      "Iter 176: error - 43.968805687696936, weights: [45.05162639  3.81447176]\n",
      "Iter 177: error - 43.96880072481354, weights: [45.05212223  3.81438185]\n",
      "Iter 178: error - 43.968796204221974, weights: [45.05259545  3.81429604]\n",
      "Iter 179: error - 43.968792086505175, weights: [45.05304709  3.81421414]\n",
      "Iter 180: error - 43.96878833575898, weights: [45.05347814  3.81413597]\n",
      "Iter 181: error - 43.96878491927901, weights: [45.05388954  3.81406138]\n",
      "Iter 182: error - 43.96878180727548, weights: [45.05428217  3.81399018]\n",
      "Iter 183: error - 43.968778972613485, weights: [45.0546569   3.81392223]\n",
      "Iter 184: error - 43.96877639057633, weights: [45.05501454  3.81385737]\n",
      "Iter 185: error - 43.968774038650146, weights: [45.05535588  3.81379548]\n",
      "Iter 186: error - 43.96877189632745, weights: [45.05568165  3.8137364 ]\n",
      "Iter 187: error - 43.96876994492838, weights: [45.05599256  3.81368002]\n",
      "Iter 188: error - 43.96876816743786, weights: [45.0562893   3.81362622]\n",
      "Iter 189: error - 43.96876654835715, weights: [45.0565725   3.81357486]\n",
      "Iter 190: error - 43.96876507356881, weights: [45.05684279  3.81352585]\n",
      "Iter 191: error - 43.96876373021349, weights: [45.05710076  3.81347907]\n",
      "Iter 192: error - 43.968762506577896, weights: [45.05734696  3.81343442]\n",
      "Iter 193: error - 43.9687613919926, weights: [45.05758194  3.81339181]\n",
      "Iter 194: error - 43.968760376739084, weights: [45.0578062   3.81335115]\n",
      "Iter 195: error - 43.968759451964864, weights: [45.05802024  3.81331234]\n",
      "Iter 196: error - 43.96875860960647, weights: [45.05822451  3.81327529]\n",
      "Iter 197: error - 43.968757842318986, weights: [45.05841947  3.81323994]\n",
      "Iter 198: error - 43.968757143412105, weights: [45.05860554  3.8132062 ]\n",
      "Iter 199: error - 43.96875650679175, weights: [45.05878313  3.813174  ]\n",
      "Iter 200: error - 43.96875592690696, weights: [45.05895262  3.81314326]\n",
      "Iter 201: error - 43.968755398701454, weights: [45.05911438  3.81311393]\n",
      "Iter 202: error - 43.968754917569576, weights: [45.05926876  3.81308594]\n",
      "Iter 203: error - 43.968754479316146, weights: [45.0594161   3.81305922]\n",
      "Iter 204: error - 43.96875408011983, weights: [45.05955673  3.81303372]\n",
      "Iter 205: error - 43.968753716499855, weights: [45.05969094  3.81300938]\n",
      "Iter 206: error - 43.968753385285666, weights: [45.05981903  3.81298615]\n",
      "Iter 207: error - 43.96875308358927, weights: [45.05994128  3.81296398]\n",
      "Iter 208: error - 43.96875280878002, weights: [45.06005796  3.81294283]\n",
      "Iter 209: error - 43.96875255846176, weights: [45.06016932  3.81292263]\n",
      "Iter 210: error - 43.968752330451835, weights: [45.06027559  3.81290336]\n",
      "Iter 211: error - 43.96875212276215, weights: [45.06037703  3.81288497]\n",
      "Iter 212: error - 43.968751933581736, weights: [45.06047383  3.81286741]\n",
      "Iter 213: error - 43.96875176126108, weights: [45.06056623  3.81285066]\n",
      "Iter 214: error - 43.968751604297616, weights: [45.0606544   3.81283467]\n",
      "Iter 215: error - 43.968751461322746, weights: [45.06073856  3.81281941]\n",
      "Iter 216: error - 43.968751331089784, weights: [45.06081888  3.81280484]\n",
      "Iter 217: error - 43.968751212463154, weights: [45.06089554  3.81279094]\n",
      "Iter 218: error - 43.968751104408526, weights: [45.06096871  3.81277768]\n",
      "Iter 219: error - 43.96875100598372, weights: [45.06103853  3.81276501]\n",
      "Iter 220: error - 43.96875091633053, weights: [45.06110517  3.81275293]\n",
      "Iter 221: error - 43.96875083466721, weights: [45.06116878  3.8127414 ]\n",
      "Iter 222: error - 43.968750760281736, weights: [45.06122948  3.81273039]\n",
      "Iter 223: error - 43.96875069252549, weights: [45.06128742  3.81271988]\n",
      "Iter 224: error - 43.96875063080767, weights: [45.06134271  3.81270986]\n",
      "Iter 225: error - 43.96875057459014, weights: [45.06139548  3.81270029]\n",
      "Iter 226: error - 43.9687505233827, weights: [45.06144585  3.81269115]\n",
      "Iter 227: error - 43.96875047673887, weights: [45.06149392  3.81268244]\n",
      "Iter 228: error - 43.968750434251945, weights: [45.06153979  3.81267412]\n",
      "Iter 229: error - 43.968750395551446, weights: [45.06158358  3.81266618]\n",
      "Iter 230: error - 43.968750360299936, weights: [45.06162537  3.8126586 ]\n",
      "Iter 231: error - 43.96875032819002, weights: [45.06166525  3.81265137]\n",
      "Iter 232: error - 43.96875029894176, weights: [45.06170331  3.81264447]\n",
      "Iter 233: error - 43.968750272300085, weights: [45.06173964  3.81263788]\n",
      "Iter 234: error - 43.96875024803271, weights: [45.06177431  3.81263159]\n",
      "Iter 235: error - 43.968750225928076, weights: [45.06180741  3.81262559]\n",
      "Iter 236: error - 43.96875020579337, weights: [45.06183899  3.81261986]\n",
      "Iter 237: error - 43.96875018745308, weights: [45.06186913  3.8126144 ]\n",
      "Iter 238: error - 43.96875017074727, weights: [45.0618979   3.81260918]\n",
      "Iter 239: error - 43.9687501555303, weights: [45.06192535  3.8126042 ]\n",
      "Iter 240: error - 43.96875014166945, weights: [45.06195156  3.81259945]\n",
      "Iter 241: error - 43.96875012904387, weights: [45.06197657  3.81259492]\n",
      "Iter 242: error - 43.96875011754349, weights: [45.06200043  3.81259059]\n",
      "Iter 243: error - 43.968750107068026, weights: [45.06202321  3.81258646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 244: error - 43.96875009752613, weights: [45.06204495  3.81258252]\n",
      "Iter 245: error - 43.96875008883461, weights: [45.0620657   3.81257875]\n",
      "Iter 246: error - 43.96875008091769, weights: [45.06208551  3.81257516]\n",
      "Iter 247: error - 43.968750073706296, weights: [45.06210441  3.81257173]\n",
      "Iter 248: error - 43.968750067137606, weights: [45.06212245  3.81256846]\n",
      "Iter 249: error - 43.9687500611543, weights: [45.06213966  3.81256534]\n",
      "Iter 250: error - 43.968750055704234, weights: [45.0621561   3.81256236]\n",
      "Iter 251: error - 43.96875005073987, weights: [45.06217178  3.81255952]\n",
      "Iter 252: error - 43.96875004621794, weights: [45.06218674  3.8125568 ]\n",
      "Iter 253: error - 43.96875004209899, weights: [45.06220103  3.81255421]\n",
      "Iter 254: error - 43.96875003834713, weights: [45.06221466  3.81255174]\n",
      "Iter 255: error - 43.96875003492965, weights: [45.06222767  3.81254938]\n",
      "Iter 256: error - 43.968750031816725, weights: [45.06224009  3.81254713]\n",
      "Iter 257: error - 43.968750028981205, weights: [45.06225194  3.81254498]\n",
      "Iter 258: error - 43.96875002639841, weights: [45.06226325  3.81254293]\n",
      "Iter 259: error - 43.96875002404579, weights: [45.06227405  3.81254097]\n",
      "Iter 260: error - 43.968750021902835, weights: [45.06228435  3.8125391 ]\n",
      "Iter 261: error - 43.968750019950846, weights: [45.06229419  3.81253732]\n"
     ]
    }
   ],
   "source": [
    "n = X.shape[1]\n",
    "\n",
    "eta = 1e-2\n",
    "max_iter = 1000\n",
    "\n",
    "W = np.array([1, 0.5])\n",
    "print(f'Number of objects = {n} \\\n",
    "       \\nLearning rate = {eta} \\\n",
    "       \\nInitial weights = {W} \\n')\n",
    "\n",
    "# список векторов весов после каждой итерации\n",
    "w_list = [W.copy()]\n",
    "    \n",
    "# список значений ошибок после каждой итерации\n",
    "errors = []\n",
    "\n",
    "# критерий сходимости (разница весов, при которой алгоритм останавливается)\n",
    "min_weight_dist = 1e-5\n",
    "\n",
    "# зададим начальную разницу весов большим числом\n",
    "weight_dist = np.inf\n",
    "\n",
    "# счетчик итераций\n",
    "iter_num = 0\n",
    "    \n",
    "# ход градиентного спуска\n",
    "while weight_dist > min_weight_dist and iter_num < max_iter:\n",
    "    y_pred = np.dot(X, W)\n",
    "    new_w = W - eta * (2 / n * np.dot(X.T, y_pred - y))\n",
    "    weight_dist = np.linalg.norm(new_w - W, ord=2)\n",
    "    error = mserror(X, new_w, y)\n",
    "    \n",
    "    w_list.append(new_w.copy())\n",
    "    errors.append(error)\n",
    "    \n",
    "    print(f'Iter {iter_num}: error - {error}, weights: {new_w}')\n",
    "    \n",
    "    iter_num += 1\n",
    "    W = new_w\n",
    "    \n",
    "w_list = np.array(w_list)\n",
    "w_pred = w_list[-1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
